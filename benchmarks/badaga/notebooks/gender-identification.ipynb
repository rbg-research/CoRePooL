{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import librosa\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the root directory\n",
    "data_directory = \"../Badaga_Corpus-v.0.1.0/\"\n",
    "tagged_file = \"Badaga-v0.1.0.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "tagged_file_path = os.path.join(data_directory, tagged_file)\n",
    "data_frame = pd.read_excel(tagged_file_path)\n",
    "\n",
    "# dropping the missing values\n",
    "data_frame.dropna(inplace=True)\n",
    "\n",
    "# loading the audio using the lambda function\n",
    "data_frame[\"audio_file_name\"] = data_frame[\"audio_file_name\"].apply(lambda x: os.path.join(data_directory, \"clips\", x))\n",
    "\n",
    "# splitting the data into train and test using the split_index from trascription\n",
    "train_df = data_frame[data_frame[\"split_label\"]!=\"test\"]\n",
    "test_df = data_frame[data_frame[\"split_label\"]==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>translated_transcript</th>\n",
       "      <th>audio_file_name</th>\n",
       "      <th>translterated_script</th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>locale</th>\n",
       "      <th>split_label</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>what is the recipe in home</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_1.mp3</td>\n",
       "      <td>manaya aena udhaka</td>\n",
       "      <td>F002</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>train</td>\n",
       "      <td>2.377187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>who is there near you</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_2.mp3</td>\n",
       "      <td>pakka dhara edhdharae</td>\n",
       "      <td>F002</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>train</td>\n",
       "      <td>2.377187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>what did you prepare for lunch</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_3.mp3</td>\n",
       "      <td>hagulu hasuga aena maditha</td>\n",
       "      <td>F002</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>train</td>\n",
       "      <td>2.351062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>did you brush</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_4.mp3</td>\n",
       "      <td>hallu ujjithaya</td>\n",
       "      <td>F002</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>train</td>\n",
       "      <td>2.194313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>did you eat</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_5.mp3</td>\n",
       "      <td>nee thindhubutaya</td>\n",
       "      <td>F002</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>train</td>\n",
       "      <td>2.272687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9832</th>\n",
       "      <td>595.0</td>\n",
       "      <td>what is price for gold</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_595.mp3</td>\n",
       "      <td>chinna baellae aesaga vario</td>\n",
       "      <td>F004</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>test</td>\n",
       "      <td>1.776375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9833</th>\n",
       "      <td>596.0</td>\n",
       "      <td>buffalo will be so black in color</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_596.mp3</td>\n",
       "      <td>yaemmae appara kappa attra</td>\n",
       "      <td>F004</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>test</td>\n",
       "      <td>1.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9834</th>\n",
       "      <td>597.0</td>\n",
       "      <td>today is my birthday</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_597.mp3</td>\n",
       "      <td>endhdhu aenna utti jaenna</td>\n",
       "      <td>F004</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>train</td>\n",
       "      <td>1.541250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9835</th>\n",
       "      <td>598.0</td>\n",
       "      <td>we are coming tomorrow.</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_598.mp3</td>\n",
       "      <td>enga naaiga bannaeyo</td>\n",
       "      <td>F004</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>train</td>\n",
       "      <td>1.724125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9836</th>\n",
       "      <td>599.0</td>\n",
       "      <td>two o clock we will go</td>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_599.mp3</td>\n",
       "      <td>aeradu manniga oppa</td>\n",
       "      <td>F004</td>\n",
       "      <td>F</td>\n",
       "      <td>ba</td>\n",
       "      <td>train</td>\n",
       "      <td>1.306125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9834 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      items              translated_transcript  \\\n",
       "0       1.0         what is the recipe in home   \n",
       "1       2.0              who is there near you   \n",
       "2       3.0     what did you prepare for lunch   \n",
       "3       4.0                      did you brush   \n",
       "4       5.0                        did you eat   \n",
       "...     ...                                ...   \n",
       "9832  595.0             what is price for gold   \n",
       "9833  596.0  buffalo will be so black in color   \n",
       "9834  597.0               today is my birthday   \n",
       "9835  598.0            we are coming tomorrow.   \n",
       "9836  599.0             two o clock we will go   \n",
       "\n",
       "                                  audio_file_name  \\\n",
       "0     ../Badaga_Corpus-v.0.1.0/clips/F002_1_1.mp3   \n",
       "1     ../Badaga_Corpus-v.0.1.0/clips/F002_1_2.mp3   \n",
       "2     ../Badaga_Corpus-v.0.1.0/clips/F002_1_3.mp3   \n",
       "3     ../Badaga_Corpus-v.0.1.0/clips/F002_1_4.mp3   \n",
       "4     ../Badaga_Corpus-v.0.1.0/clips/F002_1_5.mp3   \n",
       "...                                           ...   \n",
       "9832  ../Badaga_Corpus-v.0.1.0/clips/F004_595.mp3   \n",
       "9833  ../Badaga_Corpus-v.0.1.0/clips/F004_596.mp3   \n",
       "9834  ../Badaga_Corpus-v.0.1.0/clips/F004_597.mp3   \n",
       "9835  ../Badaga_Corpus-v.0.1.0/clips/F004_598.mp3   \n",
       "9836  ../Badaga_Corpus-v.0.1.0/clips/F004_599.mp3   \n",
       "\n",
       "             translterated_script user_id gender locale split_label  duration  \n",
       "0              manaya aena udhaka    F002      F     ba       train  2.377187  \n",
       "1           pakka dhara edhdharae    F002      F     ba       train  2.377187  \n",
       "2      hagulu hasuga aena maditha    F002      F     ba       train  2.351062  \n",
       "3                 hallu ujjithaya    F002      F     ba       train  2.194313  \n",
       "4               nee thindhubutaya    F002      F     ba       train  2.272687  \n",
       "...                           ...     ...    ...    ...         ...       ...  \n",
       "9832  chinna baellae aesaga vario    F004      F     ba        test  1.776375  \n",
       "9833   yaemmae appara kappa attra    F004      F     ba        test  1.907000  \n",
       "9834    endhdhu aenna utti jaenna    F004      F     ba       train  1.541250  \n",
       "9835         enga naaiga bannaeyo    F004      F     ba       train  1.724125  \n",
       "9836          aeradu manniga oppa    F004      F     ba       train  1.306125  \n",
       "\n",
       "[9834 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the data\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting variables such as \"audio_file_name\" and \"user_id\" as list and renaming them as \"path\" and \"label\"\n",
    "train_df[\"path\"] = list(train_df[\"audio_file_name\"])\n",
    "train_df[\"label\"] = list(train_df[\"gender\"])\n",
    "\n",
    "test_df[\"path\"] = list(test_df[\"audio_file_name\"])\n",
    "test_df[\"label\"] = list(test_df[\"gender\"])\n",
    "\n",
    "# creating a new datasets using the above list for both training and testing set\n",
    "train_df = train_df[[\"path\", \"label\"]]\n",
    "test_df = test_df[[\"path\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8365, 2), (1469, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the shape of train and test\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_1.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_2.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_3.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_4.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F002_1_5.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9830</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_593.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9831</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_594.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9834</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_597.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9835</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_598.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9836</th>\n",
       "      <td>../Badaga_Corpus-v.0.1.0/clips/F004_599.mp3</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8365 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path label\n",
       "0     ../Badaga_Corpus-v.0.1.0/clips/F002_1_1.mp3     F\n",
       "1     ../Badaga_Corpus-v.0.1.0/clips/F002_1_2.mp3     F\n",
       "2     ../Badaga_Corpus-v.0.1.0/clips/F002_1_3.mp3     F\n",
       "3     ../Badaga_Corpus-v.0.1.0/clips/F002_1_4.mp3     F\n",
       "4     ../Badaga_Corpus-v.0.1.0/clips/F002_1_5.mp3     F\n",
       "...                                           ...   ...\n",
       "9830  ../Badaga_Corpus-v.0.1.0/clips/F004_593.mp3     F\n",
       "9831  ../Badaga_Corpus-v.0.1.0/clips/F004_594.mp3     F\n",
       "9834  ../Badaga_Corpus-v.0.1.0/clips/F004_597.mp3     F\n",
       "9835  ../Badaga_Corpus-v.0.1.0/clips/F004_598.mp3     F\n",
       "9836  ../Badaga_Corpus-v.0.1.0/clips/F004_599.mp3     F\n",
       "\n",
       "[8365 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the data\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the index for the newly created dataset\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# saving it as csv files for both training and testing\n",
    "train_df.to_csv(\"files/gender_train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "test_df.to_csv(\"files/gender_test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to specify the input and output column\n",
    "input_column = \"path\"\n",
    "output_column = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fac184f4146374a1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/ubuntu/.cache/huggingface/datasets/csv/default-fac184f4146374a1/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae7d6a02a594f849a45fc6562274802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f29d1685e444feebcdfd30a761eba67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/csv/default-fac184f4146374a1/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3c5c78de5c421eb638acce8f0277c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['path', 'label'],\n",
      "    num_rows: 8365\n",
      "})\n",
      "Dataset({\n",
      "    features: ['path', 'label'],\n",
      "    num_rows: 1469\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading the created dataset using datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"files/gender_train.csv\", \n",
    "    \"validation\": \"files/gender_test.csv\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classification problem with 2 classes: ['F', 'M']\n"
     ]
    }
   ],
   "source": [
    "# we need to distinguish the unique labels \n",
    "label_list = train_dataset.unique(output_column)\n",
    "label_list.sort()  # Let's sort it for determinism\n",
    "num_labels = len(label_list)\n",
    "print(f\"A classification problem with {num_labels} classes: {label_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the feature extractor and processor from the transformers\n",
    "from transformers import AutoConfig, Wav2Vec2Processor, Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Wav2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model_name_or_path = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "pooling_mode = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id={label: i for i, label in enumerate(label_list)},\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = feature_extractor.sampling_rate\n",
    "print(f\"The target sampling rate: {target_sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the audio data using librosa\n",
    "import librosa\n",
    "def speech_file_to_array_fn(path):\n",
    "    a, s = librosa.load(path, sr=16000)\n",
    "#     speech_array, sampling_rate = torchaudio.load(path)\n",
    "#     resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "#     speech = resampler(speech_array).squeeze().numpy()\n",
    "    return a\n",
    "\n",
    "def label_to_id(label, label_list):\n",
    "\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "\n",
    "    return label\n",
    "\n",
    "# function for pre-processing \n",
    "def preprocess_function(examples):\n",
    "    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n",
    "    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n",
    "\n",
    "    result = feature_extractor(speech_list, sampling_rate=target_sampling_rate)\n",
    "    result[\"labels\"] = list(target_list)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using map function to map the pre-processed files to the train adn test sets\n",
    "import torchaudio\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dataclass for speech classifier\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Wav2Vec Pretrained Model for Gender-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the classifier class\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function for datacollator and padding\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`)\n",
    "            The feature_extractor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting only for classification (regression is set)\n",
    "is_regression = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for computing metrics for evaluation\n",
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "\n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForSpeechClassification: ['quantizer.weight_proj.weight', 'project_q.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_hid.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# classification\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the arguments for training \n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"wav2vec2-rbg-badaga-gender\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1.0,\n",
    "    fp16=True,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the trainer function\n",
    "from typing import Any, Dict, Union\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        if self.use_amp:\n",
    "            with autocast():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "        else:\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the data collator with padding\n",
    "data_collator = DataCollatorCTCWithPadding(feature_extractor=feature_extractor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# setting up CTCT trainer\n",
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running training *****\n",
      "  Num examples = 8365\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1046\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='1046' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 170/1046 20:23 < 1:46:21, 0.14 it/s, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.752157</td>\n",
       "      <td>0.499660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>0.693021</td>\n",
       "      <td>0.499660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.689847</td>\n",
       "      <td>0.527570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.389900</td>\n",
       "      <td>0.110088</td>\n",
       "      <td>0.994554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>1.056181</td>\n",
       "      <td>0.691627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.677600</td>\n",
       "      <td>0.116207</td>\n",
       "      <td>0.974132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.115895</td>\n",
       "      <td>0.974132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.005484</td>\n",
       "      <td>0.998639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.015810</td>\n",
       "      <td>0.995916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.024722</td>\n",
       "      <td>0.994554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.024822</td>\n",
       "      <td>0.995235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.999319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>0.999319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.007541</td>\n",
       "      <td>0.998639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.998639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.999319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-10\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-10/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-10/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-10/preprocessor_config.json\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-20\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-20/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-20/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-20/preprocessor_config.json\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-30\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-30/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-30/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-30/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-10] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-40\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-40/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-40/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-40/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-20] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-50\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-50/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-50/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-50/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-30] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-60\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-60/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-60/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-60/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-40] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-70\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-70/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-70/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-70/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-50] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-80\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-80/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-80/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-80/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-60] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-90\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-90/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-90/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-90/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-70] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-100\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-100/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-100/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-100/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-80] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-110\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-110/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-110/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-110/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-90] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-120\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-120/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-120/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-120/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-100] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-130\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-130/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-130/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-130/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-110] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-140\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-140/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-140/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-140/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-120] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-150\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-150/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-150/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-150/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-130] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1469\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to wav2vec2-rbg-badaga-gender/checkpoint-160\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-160/config.json\n",
      "Model weights saved in wav2vec2-rbg-badaga-gender/checkpoint-160/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-rbg-badaga-gender/checkpoint-160/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-rbg-badaga-gender/checkpoint-140] due to args.save_total_limit\n",
      "/home/ubuntu/environments/corepool/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1055: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imporing the libaries \n",
    "import librosa\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting with cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the saved checkpoint to predict \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name_or_path = \"wav2vec2-base-rbg-badaga-gender/checkpoint-1045\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for predicting the result\n",
    "def predict(batch):\n",
    "    features = processor(batch[\"speech\"], sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits \n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    batch[\"predicted\"] = pred_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping speech file as array\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_dataset.map(predict, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining and printing the label names\n",
    "label_names = [config.id2label[i] for i in range(config.num_labels)]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the results\n",
    "y_true = [config.label2id[name] for name in result[\"emotion\"]]\n",
    "y_pred = result[\"predicted\"]\n",
    "\n",
    "print(y_true[:5])\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the classification report\n",
    "print(classification_report(y_true, y_pred, target_names=label_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp",
   "language": "python",
   "name": "corepool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
